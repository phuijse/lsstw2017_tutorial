{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features were computed offline using the procedure described above. Here we load the features for 10,000 VVV light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_periods = pickle.load(open(\"data/lc_periods.pkl\", \"rb\"))\n",
    "data_P = pickle.load(open(\"data/features_P.pkl\", \"rb\"))\n",
    "data_U = pickle.load(open(\"data/features_U.pkl\", \"rb\"))\n",
    "\n",
    "feature_names = list()\n",
    "for i in range(40):\n",
    "    feature_names.append(\"GP%d\" %(i+1))\n",
    "feature_names.append('NMSE')\n",
    "feature_names.append('NSFD')\n",
    "feature_names.append('Median')\n",
    "feature_names.append('IQR')\n",
    "feature_names.append('Skewness')\n",
    "feature_names.append('Kurtosis')\n",
    "feature_names.append('Freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 2))\n",
    "for i in range(7):\n",
    "    ax = fig.add_subplot(1, 7, i+1)\n",
    "    ax.hist(data_U[:, -i-1])\n",
    "    ax.set_title(feature_names[-i-1])\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('U set')\n",
    "plt.tight_layout(pad=0.1)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 2))\n",
    "for i in range(7):\n",
    "    ax = fig.add_subplot(1, 7, i+1)\n",
    "    ax.hist(data_P[:, -i-1])\n",
    "    ax.set_title(feature_names[-i-1])\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('P set')\n",
    "plt.tight_layout(pad=0.1)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 2))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "class_mu = np.mean(data_U[:, :40], axis=0)\n",
    "class_std = np.std(data_U[:, :40], axis=0)\n",
    "ax.plot(range(80), np.tile(class_mu, 2))\n",
    "ax.fill_between(range(80), np.tile(class_mu - 2*class_std,2), np.tile(class_mu + 2*class_std,2), alpha=0.5)\n",
    "ax.invert_yaxis()\n",
    "ylims = ax.get_ylim()\n",
    "plt.ylabel(\"GP fit\")\n",
    "plt.title(\"Average fit in U set\")\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "class_mu = np.mean(data_P[:, :40], axis=0)\n",
    "class_std = np.std(data_P[:, :40], axis=0)\n",
    "ax.plot(range(80), np.tile(class_mu, 2))\n",
    "ax.fill_between(range(80), np.tile(class_mu - 2*class_std,2), np.tile(class_mu + 2*class_std,2), alpha=0.5)\n",
    "ax.invert_yaxis()\n",
    "ax.set_ylim(ylims)\n",
    "plt.title(\"Average fit in P set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transductive Positive-Unlabeled (PU) learning\n",
    "\n",
    "PU learning is a special case of semi-supervised learning. The input to PU methods are two datasets: P and U. P is a dataset of known objects of one particular category (in this case the RR Lyrae stars). U is an unlalebed set that may contain elements of P and of many other categories. The objective is to recover all the examples of U that are similar to P. In this case we train a random forest where bootstrap is only used in the U set. Each tree gives a probability to the out-of-bag (oob) samples (points that were not used to train). The average oob prediction is our transductive label for the elements of U."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "T = 1000  # number of trees\n",
    "NP = data_P.shape[0]\n",
    "NU = data_U.shape[0]\n",
    "K = 2*NP  # Size of the bootstrap \n",
    "Y = np.zeros(shape=(NP+K,))\n",
    "Y[:NP] = 1.0\n",
    "M = data_U.shape[1]\n",
    "\n",
    "tic = time.time()\n",
    "n_oob = np.zeros(shape=(NU,))\n",
    "f_oob = np.zeros(shape=(NU, 2))\n",
    "feature_importance = np.zeros(shape=(M,))\n",
    "models = []\n",
    "\n",
    "# Create forest\n",
    "for i in range(T):\n",
    "    models.append(DecisionTreeClassifier(max_depth=None, max_features='sqrt', criterion='entropy'))\n",
    "\n",
    "def parallel_transduce_tree(t):\n",
    "    # Bootstrap resample\n",
    "    b = np.random.choice(np.arange(NU), replace=True, size=K)\n",
    "    data_bootstrap = np.concatenate((data_P, data_U[b, :]), axis=0)\n",
    "    # Train tree\n",
    "    t.fit(data_bootstrap, Y)\n",
    "    # Predict in oob\n",
    "    idx_oob = sorted(set(range(NU)) - set(np.unique(b)))\n",
    "    return idx_oob, t.predict_proba(data_U[idx_oob]), t.feature_importances_\n",
    "\n",
    "ans = Parallel(n_jobs=4, backend='threading')(\n",
    "    delayed(parallel_transduce_tree)(t) for t in models)\n",
    "\n",
    "for idx_oob, prediction, fimportance in ans:\n",
    "    f_oob[idx_oob] += prediction\n",
    "    n_oob[idx_oob] += 1\n",
    "    feature_importance += fimportance\n",
    "\n",
    "probs = f_oob/n_oob[:, np.newaxis]\n",
    "feature_importance = feature_importance/T\n",
    "\n",
    "print(\"Elapsed time: %0.2f [s]\" %(time.time()-tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance from the transductive random forest\n",
    "\n",
    "From the random forest we can recover the importance of the features. The importance comes from the metric that was used to do the feature splits on each tree. In this case we used the information gain (difference in information entropy before and after doing the split). The higher the information gain the higher that the feature is on the tree. If a feature is up in all the trees it is an indicator that it is an important feature. Note that more sofisticated feature selection schemes would also measure the redundancy and/or synergy between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.bar(np.arange(len(feature_importance)), feature_importance[fidx], alpha=0.5, width=1.0)\n",
    "plt.xticks(np.arange(M)+0.5, np.array(feature_names)[fidx], rotation='vertical')\n",
    "plt.ylabel('Feature importance')\n",
    "plt.grid()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3.5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "avg_lc = np.average(data_P[:, :40], axis=0)\n",
    "sp = ax.scatter(np.linspace(0, 1, num=40), avg_lc, s=400, c=feature_importance[:40], \n",
    "                linewidth=0, alpha=0.6, cmap=plt.cm.Spectral_r)\n",
    "plt.xlabel('Phase')\n",
    "plt.ylabel('Normalized magnitude')\n",
    "plt.colorbar(sp, label='Importance')\n",
    "plt.grid()\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency (period) is the most important feature. The points around $\\phi=0.2$ are also important as this highlights the characteristic assymetry in the RR Lyrae light curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
