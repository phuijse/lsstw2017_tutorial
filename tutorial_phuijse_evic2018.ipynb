{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container {\n",
    "    width:80% ! important;\n",
    "}\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    ".rendered_html { \n",
    "    font-size:1.0em; \n",
    "}\n",
    ".rendered_html table{\n",
    "    width: 80%;\n",
    "    margin-left:auto; \n",
    "    margin-right:auto;\n",
    "    padding: 20px;\n",
    "    border: 0px solid black;    \n",
    "    background-color: #ff;\n",
    "}\n",
    ".rendered_html td, .rendered_html th \n",
    "{\n",
    "    vertical-align: top;\n",
    "    text-align: left;\n",
    "    font-size: 14px;\n",
    "    font-face: sans-serif;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h1> Analysis and Classification of Periodic Variable Stars</h1>\n",
    "<h2>Pablo Huijse H. (phuijse at inf dot uach dot cl)</h2>\n",
    "<h3>Universidad Austral de Chile & Millennium Institute of Astrophysics</h3>\n",
    "</center>\n",
    "\n",
    "LIVE at https://github.com/phuijse/tutorial_periodic_stars\n",
    "\n",
    "Thanks to\n",
    "- The organizers\n",
    "- The Millennium Institute of Astrophysics\n",
    "- CONICYT FONDECYT 1170305 and PAI 79170017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variable stars\n",
    "\n",
    "- Stars whose brightness change in time\n",
    "- Different reasons behind this\n",
    "\n",
    "### Pulsating variables\n",
    "- Some variable stars pulsate radially\n",
    "- They expand/heat and contract/cool regularly\n",
    "- Examples: Cepheid and RR Lyrae\n",
    "\n",
    "<center>\n",
    "<a href=\"https://www.youtube.com/watch?v=sXJBrRmHPj8\">\n",
    "    <img src=\"https://media.giphy.com/media/QP4taxvfVmVEI/giphy.gif\" width=\"400\">\n",
    "</a>\n",
    "</center>\n",
    "\n",
    "### Eclipsing Binaries\n",
    "\n",
    "- System of two stars\n",
    "- The rotational plane is aligned with us\n",
    "- From our point of view we see brightness decrease with the mutual eclipses\n",
    "<center>\n",
    "<table>\n",
    "    <tr><td>\n",
    "        <a href=\"http://www.physast.uga.edu/~rls/astro1020/ch16/ovhd.html\">\n",
    "            <img src=\"img/intro-eb.gif\" width=\"300\">\n",
    "        </a>\n",
    "    </td>\n",
    "    <td>\n",
    "        <a href=\"https://en.wikipedia.org/wiki/File:Algol_AB_movie_imaged_with_the_CHARA_interferometer_-_labeled.gif\">\n",
    "            <img src=\"https://media.giphy.com/media/aYb0Ob2GHJ280/giphy.gif\" width=\"300\">\n",
    "        </a>\n",
    "    </td></tr>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scientific motivation\n",
    "***\n",
    "\n",
    "- Variable stars as distance tracers: Milky-way maps\n",
    "<table>\n",
    "    <tr><td>   \n",
    "        <img src=\"img/period-luminosity-relation.gif\" width=\"400\">\n",
    "    </td><td>\n",
    "        <img src=\"img/intro-milky-way.jpg\" width=\"400\">\n",
    "    </td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "- Variable star analysis and classification: **Astrophysics**\n",
    "<center>\n",
    "<a href=\"http://www.atnf.csiro.au/outreach/education/senior/astrophysics/variable_types.html\">\n",
    "    <img src=\"img/variable-star-classification.gif\" width=\"400\">\n",
    "</a>\n",
    "</center>\n",
    "- New methods to analyze astronomical data: **Signal processing** and **Data Science**\n",
    "    - Room for interdisciplinary research\n",
    "    - Astroinformatics and Astrostatistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Light curve\n",
    "***\n",
    "\n",
    "- Time series of a star's flux (brightness) on a given passband\n",
    "- The \"apparent\" brightness is estimated through **Photometry**\n",
    "- Variable stars are studied through their light curves\n",
    "\n",
    "<table><tr><td>\n",
    "    <img src=\"img/intro-vista.png\" width=\"250\">\n",
    "</td><td>\n",
    "    <img src=\"img/intro-sources.png\" width=\"300\">\n",
    "</td></tr></table>\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/intro-sources-time.png\" width=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import rcParams, animation\n",
    "rcParams.update({'font.size': 12})\n",
    "rcParams.update({'axes.grid': True})\n",
    "\n",
    "# Get some light curves to play\n",
    "with gzip.open(\"data/lc_data.pgz\", mode=\"r\") as f:\n",
    "    lc_data = pickle.load(f)\n",
    "\n",
    "lc_periods = pickle.load(open(\"data/lc_periods.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Inspecting a light curve\n",
    "\n",
    "In this case light curves are text files with three colums\n",
    "- **Modified Julian Data (MJD):** Corresponds to time \n",
    "- **Magnitude:** Corresponds to apparent brightness (log scale)\n",
    "- **Error:** Photometric error estimation of the magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 4), tight_layout=True)\n",
    "mjd, mag, err = lc_data[6].T\n",
    "ax.errorbar(mjd, mag, err, fmt='o')\n",
    "ax.invert_yaxis(); \n",
    "ax.set_xlabel('Modified Julian Date (MJD)\\n ')\n",
    "ax.set_ylabel('Magnitude\\n(The smaller the brighter)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Irregular sampling, data gaps\n",
    "- Heteroscedastic noise: Error variance change in time\n",
    "\n",
    "This light curve is actually from a periodic variable star..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def fold(time, period):\n",
    "    \"\"\"\n",
    "    returns phase = time/period - floor(time/period)\n",
    "    \"\"\"\n",
    "    return np.mod(time, period)/period\n",
    "\n",
    "idx = 6\n",
    "mjd, mag, err = lc_data[idx].T\n",
    "fig, ax = plt.subplots(figsize=(9, 4), tight_layout=True)\n",
    "phi = fold(mjd, lc_periods[idx])\n",
    "ax.errorbar(np.hstack((phi, phi+1)), \n",
    "            np.hstack((mag, mag)), \n",
    "            np.hstack((err, err)), fmt='o')\n",
    "ax.invert_yaxis(); \n",
    "ax.set_ylabel('Magnitude\\n(The smaller the brighter)');\n",
    "ax.set_xlabel('Phase @ Period %0.6f' %(lc_periods[idx]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Folding the light curve\n",
    "\n",
    "- Technique used by astronomers to visually inspect periodic variables\n",
    "- You need a candidate period $P$ to perform the folding\n",
    "- The time axis is divided in chucks of size $P$ and plotted on top each other\n",
    "\n",
    "$$\n",
    "\\phi = \\text{modulo}(\\text{MJD}, P)/P\n",
    "$$\n",
    "- Then you plot the magnitude as a function of $\\phi$ \n",
    "    - If $P$ is close to the true period:  Nice periodic shape\n",
    "    - Otherwise: Noisy pattern\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 4), tight_layout=True)\n",
    "period_grid = np.linspace(lc_periods[6]-0.001, lc_periods[6]+0.001, num=100)\n",
    "phi = fold(mjd, period_grid[0])\n",
    "line, caps, errorbars = ax.errorbar(np.hstack((phi, phi+1)), \n",
    "                                    np.hstack((mag, mag)), \n",
    "                                    np.hstack((err, err)), fmt='o')\n",
    "segs = errorbars[0].get_segments()\n",
    "ax.invert_yaxis(); \n",
    "ax.set_ylabel('Magnitude\\n(The smaller the brighter)');\n",
    "\n",
    "def update(n):\n",
    "    phi = fold(mjd, period_grid[n])\n",
    "    for i in range(len(segs)//2):\n",
    "        segs[i][:, 0] = phi[i]\n",
    "        segs[i+len(phi)][:, 0] = phi[i]+1\n",
    "\n",
    "    line.set_xdata(np.hstack((phi, phi+1)))\n",
    "    errorbars[0].set_segments(segs)\n",
    "    ax.set_xlabel('Phase @ Period %0.6f' %(period_grid[n]))\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update, frames=100, interval=100, repeat=False, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Periodogram\n",
    "\n",
    "- We want to find the period (fundamental frequency) of the star\n",
    "- This is generally done using the **Fourier transform** (FT) or correlation\n",
    "- FT and correlation assumme regular time sampling\n",
    "- Estimating the period with irregular sampling\n",
    "    1. Least squares: Lomb-Scargle periodogram\n",
    "    1. ANOVA periodogram\n",
    "    1. Conditional Entropy and **Mutual Information** periodograms\n",
    "    1. ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!pip install P4J --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import P4J\n",
    "my_per = P4J.periodogram(method='QMIEU') \n",
    "mjd, mag, err = lc_data[6].T\n",
    "my_per.set_data(mjd, mag, err, h_KDE_P=0.2)\n",
    "my_per.frequency_grid_evaluation(fmin=0.0, fmax=4.0, fresolution=1e-4)\n",
    "my_per.finetune_best_frequencies(fresolution=1e-5, n_local_optima=10)\n",
    "freq, per = my_per.get_periodogram()\n",
    "fbest, pbest  = my_per.get_best_frequencies()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4), tight_layout=True)\n",
    "ax.plot(freq, per)\n",
    "ax.set_xlabel('Frequency [1/MJD]')\n",
    "ax.set_ylabel('Periodogram')\n",
    "print(\"Best period: %f days\" %(1.0/fbest[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 4), tight_layout=True)\n",
    "phi = fold(mjd, 1.0/fbest[0])\n",
    "ax.errorbar(np.hstack((phi, phi+1)), \n",
    "            np.hstack((mag, mag)), \n",
    "            np.hstack((err, err)), fmt='o')\n",
    "ax.invert_yaxis(); \n",
    "ax.set_ylabel('Magnitude\\n(The smaller the brighter)');\n",
    "ax.set_xlabel('Phase @ Period %0.6f' %(1.0/fbest[0]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting features from our periodic light curves\n",
    "\n",
    "- We want to train a neural network to discriminate a particular type of star: **RR Lyrae** \n",
    "- Given that we have the period we train on the folded light curve\n",
    "- We will normalize and interpolate the folded light curve using kernel regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_lc(lc_data, period, phi_interp, sp=0.15): \n",
    "    mjd, mag, err = lc_data.T\n",
    "    phi = np.mod(mjd, period)/period\n",
    "    mag_interp = np.zeros_like(phi_interp)\n",
    "    err_interp = np.zeros_like(phi_interp)\n",
    "    w = 1.0/err**2\n",
    "    for i in range(len(phi_interp)):\n",
    "        gt = np.exp((np.cos(2.0*np.pi*(phi_interp[i] - phi)) -1)/sp**2)\n",
    "        norm = np.sum(w*gt)\n",
    "        mag_interp[i] = np.sum(w*gt*mag)/norm\n",
    "        err_interp[i] = np.sqrt(np.sum(w*gt*(mag - mag_interp[i])**2)/norm)\n",
    "    err_interp += np.sqrt(np.median(err**2))\n",
    "    idx_max =  np.argmin(mag_interp)\n",
    "    mag_interp = np.roll(mag_interp, -idx_max)\n",
    "    err_interp = np.roll(err_interp, -idx_max)\n",
    "    max_val = np.amax(mag_interp + err_interp)\n",
    "    min_val = np.amin(mag_interp - err_interp)\n",
    "    mag_interp = 2*(mag_interp - min_val)/(max_val - min_val) - 1\n",
    "    err_interp = 2*err_interp/(max_val - min_val)\n",
    "    return mag_interp, err_interp, [max_val, min_val, idx_max]\n",
    "\n",
    "def defeaturize_lc(mag, err, norm):\n",
    "    # center, scale, idx_max = norm[0], norm[1], norm[2]\n",
    "    max_val, min_val, idx_max = norm[0], norm[1], norm[2]\n",
    "    idx_max = int(idx_max)\n",
    "    return 0.5*(np.roll(mag, idx_max) +1)*(max_val - min_val) + min_val, 0.5*np.roll(err, idx_max)*(max_val - min_val)\n",
    "\n",
    "\n",
    "phi_interp = np.linspace(0, 1, num=40)\n",
    "features = np.zeros(shape=(len(lc_data), len(phi_interp)))\n",
    "weights = np.zeros(shape=(len(lc_data), len(phi_interp)))\n",
    "norm = np.zeros(shape=(len(lc_data), 3))\n",
    "for i in range(len(lc_data)):\n",
    "    features[i, :], weights[i, :], norm[i, :] = featurize_lc(lc_data[i], lc_periods[i], phi_interp)\n",
    " \n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "lc_dataset = TensorDataset(torch.from_numpy(features.astype('float32')), \n",
    "                           torch.from_numpy(weights.astype('float32')),\n",
    "                           torch.from_numpy(lc_periods.astype('float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Button\n",
    "\n",
    "next_button = Button(description=\"Next\")\n",
    "idx = 4950\n",
    "fig, ax = plt.subplots(figsize=(6, 5), tight_layout=True)\n",
    "\n",
    "def plot_features(idx):\n",
    "    ax.cla(); sample = lc_dataset.__getitem__(idx)\n",
    "    ax.set_title(\"Idx: %d\\nPeriod: %0.6f\" %(idx, sample[2]))\n",
    "    mag, err = defeaturize_lc(sample[0][:40], sample[1][:40], norm[idx])\n",
    "    #mag, err = sample[0][:40].numpy(), sample[1][:40].numpy()\n",
    "    ax.plot(phi_interp, mag, lw=4)\n",
    "    ax.fill_between(phi_interp, (mag-err), (mag+err), alpha=0.5)\n",
    "    ax.set_xlabel('Phase'); ax.set_ylabel('Normalized magnitude');\n",
    "    mjd, mag, err = lc_data[idx][:, 0], lc_data[idx][:, 1], lc_data[idx][:, 2]\n",
    "    phi = fold(mjd, sample[2])\n",
    "    ax.errorbar(phi, mag, err, fmt='.', c='k', alpha=0.5, label='data'); \n",
    "    ax.invert_yaxis()\n",
    "    plt.legend();\n",
    "\n",
    "def on_nbutton_clicked(b):\n",
    "    global idx\n",
    "    idx += 1\n",
    "    plot_features(idx)\n",
    "                \n",
    "next_button.on_click(on_nbutton_clicked)\n",
    "plot_features(idx)\n",
    "next_button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Trainining a variational autoencoder\n",
    "\n",
    "1. In this part we will train an [autoencoder](https://docs.google.com/presentation/d/1IJ2n8X4w8pvzNLmpJB-ms6-GDHWthfsJTFuyUqHfXg8/edit?usp=sharing) to visualize the feature space \n",
    "- We will use [PyTorch](https://pytorch.org/) to create and train the model\n",
    "- We have light curves with unknown label and 50 light curves labeled as **RR Lyrae**\n",
    "- Can we find unlabeled light curves that belong to the RR Lyrae class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def logsumexp(inputs, dim=None, keepdim=True):    \n",
    "    # From: https://github.com/YosefLab/scVI/issues/13\n",
    "    return (inputs - F.log_softmax(inputs, dim=dim)).sum(dim, keepdim=keepdim)\n",
    "\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, n_input=40, n_hidden=64, n_latent=2, importance_sampling=False):\n",
    "        super(VAE, self).__init__()\n",
    "        self.importance = importance_sampling\n",
    "        # Encoder layers\n",
    "        self.enc_hidden = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.enc_mu = torch.nn.Linear(n_hidden, n_latent)\n",
    "        self.enc_logvar = torch.nn.Linear(n_hidden, n_latent)\n",
    "        # decoder layers\n",
    "        self.dec_hidden = torch.nn.Linear(n_latent, n_hidden) \n",
    "        self.dec_mu = torch.nn.Linear(n_hidden, n_input)\n",
    "        self.dec_logvar = torch.nn.Linear(n_hidden, 1)\n",
    "\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.enc_hidden(x))\n",
    "        return self.enc_mu(h), self.enc_logvar(h)\n",
    "\n",
    "    def sample(self, mu, logvar, k=1):\n",
    "        batch_size, n_latent = logvar.shape\n",
    "        std = (0.5*logvar).exp()\n",
    "        eps = torch.randn(batch_size, k, n_latent, device=std.device, requires_grad=False)\n",
    "        return eps.mul(std.unsqueeze(1)).add(mu.unsqueeze(1))\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.dec_hidden(z))\n",
    "        hatx, hatlogvar = self.dec_mu(h), self.dec_logvar(h)\n",
    "        return hatx, hatlogvar        \n",
    "\n",
    "    def forward(self, x, k=1):\n",
    "        enc_mu, enc_logvar = self.encode(x)\n",
    "        z = self.sample(enc_mu, enc_logvar, k)\n",
    "        dec_mu, dec_logvar = self.decode(z)\n",
    "        return dec_mu, enc_mu, enc_logvar, dec_logvar, z\n",
    "    \n",
    "    def ELBO(self, x, w, mc_samples=1):  \n",
    "        dec_mu, enc_mu, enc_logvar, dec_logvar, z = self.forward(x, mc_samples)\n",
    "        #logpxz = -0.5*(2.*torch.log(w.unsqueeze(1) + (dec_logvar/2).exp()) \\\n",
    "        #               + (x.unsqueeze(1) - dec_mu).pow(2)/(dec_logvar.exp() + w.pow(2).unsqueeze(1))).sum(dim=-1)    \n",
    "        logpxz = -0.5*(dec_logvar + (x.unsqueeze(1) - dec_mu).pow(2)/dec_logvar.exp()).sum(dim=-1)    \n",
    "        \n",
    "        if self.importance: # Importance-Weighted autoencoder (IWAE)\n",
    "            logqzxpz = 0.5 * (z.pow(2) - z.sub(enc_mu.unsqueeze(1)).pow(2)/enc_logvar.unsqueeze(1).exp() - enc_logvar.unsqueeze(1)).sum(dim=-1)\n",
    "        else:  # Variational autoencoder\n",
    "            logqzxpz = -0.5 * (1.0 + enc_logvar - enc_mu.pow(2) - enc_logvar.exp()).sum(dim=-1).unsqueeze_(1)\n",
    "        ELBO = torch.sum(logsumexp(logqzxpz - logpxz, dim=1) + np.log(mc_samples))\n",
    "        return ELBO, logpxz.sum()/mc_samples, logqzxpz.sum()/logqzxpz.shape[1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from vae import live_metric_plotter\n",
    "\n",
    "batch_size_, nepochs, mc_samples = 8, 50, 32\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "P = np.random.permutation(5000)\n",
    "train_loader = DataLoader(dataset=lc_dataset, batch_size=batch_size_, \n",
    "                          sampler=torch.utils.data.SubsetRandomSampler(P[:3000]))\n",
    "valid_loader = DataLoader(dataset=lc_dataset, batch_size=batch_size_, \n",
    "                          sampler=torch.utils.data.SubsetRandomSampler(P[3000:]))\n",
    "\n",
    "model = VAE(n_input=40, n_hidden=20, n_latent=2, importance_sampling=True)\n",
    "print(model)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print(\"Number of trainable parameters: %d\" %(sum([np.prod(p.size()) for p in model_parameters])))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "plotter = live_metric_plotter(figsize=(9, 4))\n",
    "metrics = np.zeros(shape=(nepochs, 2, 2))\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    # Train \n",
    "    for feature, weight, period in train_loader:\n",
    "        optimizer.zero_grad()        \n",
    "        loss, rec_loss, reg_loss = model.ELBO(feature, weight, mc_samples)        \n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)        \n",
    "        optimizer.step()\n",
    "        metrics[epoch, 0, 0] += rec_loss.detach().numpy()/len(train_loader.dataset)\n",
    "        metrics[epoch, 0, 1] += reg_loss.detach().numpy()/len(train_loader.dataset)\n",
    "    # Test\n",
    "    for feature, weight, period in valid_loader:\n",
    "        loss, rec_loss, reg_loss = model.ELBO(feature, weight, mc_samples)\n",
    "        metrics[epoch, 1, 0] += rec_loss.detach().numpy()/len(valid_loader.dataset)\n",
    "        metrics[epoch, 1, 1] += reg_loss.detach().numpy()/len(valid_loader.dataset)\n",
    "    \n",
    "    if epoch > 0:\n",
    "        plotter.update(epoch, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dec_mu, test_enc_mu, test_enc_logvar, test_dec_logvar, test_z = model.forward(lc_dataset.tensors[0], k=10)\n",
    "test_enc_mu, test_enc_sigma = test_enc_mu.detach().numpy(), (test_enc_logvar.detach()*0.5).exp().numpy()\n",
    "test_dec_mu = test_dec_mu.detach().numpy()\n",
    "\n",
    "\"\"\"\n",
    "The figure shows the digits in latent space as a dot (mean of the variational posterior) with \n",
    "errorbars (standard deviation of the variational posterior). Each point is a distribution!\n",
    "\"\"\"\n",
    "fig = plt.figure(figsize=(10, 5), dpi=80)\n",
    "ax_main = plt.subplot2grid((2, 3), (0, 0), colspan=2, rowspan=2)\n",
    "ax_ori = plt.subplot2grid((2, 3), (0, 2))\n",
    "ax_rec = plt.subplot2grid((2, 3), (1, 2))\n",
    "a, b, c = ax_main.errorbar(x=test_enc_mu[:, 0], y=test_enc_mu[:, 1], \n",
    "                           xerr=test_enc_sigma[:, 0], yerr=test_enc_sigma[:, 1], \n",
    "                           fmt='none', alpha=0.2, zorder=-1)\n",
    "labels = np.zeros(shape=(5000,)); labels[4950:] = 1\n",
    "\n",
    "sc = ax_main.scatter(test_enc_mu[:, 0], test_enc_mu[:, 1], s=2, alpha=0.2, \n",
    "                     c=labels, cmap=plt.cm.RdBu_r)\n",
    "clb = plt.colorbar(sc, ax=ax_main)\n",
    "for i in range(2):\n",
    "    c[i].set_color(clb.to_rgba(labels))\n",
    "    \n",
    "    \n",
    "c_lim, r_lim = ax_main.get_xlim(), ax_main.get_ylim()\n",
    "plt.tight_layout()\n",
    "phi_interp = np.linspace(0, 1, num=40)\n",
    "def onclick(event):\n",
    "    z_closest = [event.xdata, event.ydata]\n",
    "    print(z_closest)\n",
    "    idx = np.argmin(np.sum((test_enc_mu[:, :2] - z_closest)**2, axis=1))\n",
    "    ax_ori.cla(); ax_ori.set_title(\"Idx:%d, Label:%d\" %(idx, labels[idx]))\n",
    "    mjd, mag, err = lc_data[idx].T\n",
    "    phi = fold(mjd, lc_dataset.tensors[2][idx])\n",
    "    ax_ori.errorbar(phi, mag, err, c='k', fmt='.')\n",
    "    mag, err = defeaturize_lc(lc_dataset.tensors[0][idx].numpy(), \n",
    "                              lc_dataset.tensors[1][idx].numpy(), norm[idx])\n",
    "    ax_ori.plot(phi_interp, mag, lw=2)\n",
    "    ax_ori.fill_between(phi_interp, mag - err, mag + err, alpha=0.5)\n",
    "    ax_ori.invert_yaxis(); \n",
    "    ax_rec.cla(); ax_rec.invert_yaxis(); #ax_rec.set_ylim([2.5, -1.5]); \n",
    "    mag, err = lc_dataset.tensors[0][idx].numpy(), lc_dataset.tensors[1][idx].numpy()\n",
    "    ax_rec.plot(phi_interp, mag, lw=2)\n",
    "    ax_rec.fill_between(phi_interp, mag - err, mag + err, alpha=0.5)\n",
    "    mu_dec = np.mean(test_dec_mu[idx], axis=0)\n",
    "    s_dec = np.std(test_dec_mu[idx], axis=0)\n",
    "    ax_rec.plot(phi_interp, mu_dec, c='r', lw=2)\n",
    "    ax_rec.fill_between(phi_interp, mu_dec-2*s_dec, mu_dec+2*s_dec, facecolor='r', alpha=0.5)\n",
    "    \n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
